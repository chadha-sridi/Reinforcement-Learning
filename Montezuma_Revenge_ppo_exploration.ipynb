{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import csv\n",
    "import re \n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import cv2\n",
    "from hashlib import sha256\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecEnvWrapper\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:563: UserWarning: \u001b[33mWARN: Using the latest versioned environment `MontezumaRevenge-v4` instead of the unversioned environment `MontezumaRevenge`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = gym.make('MontezumaRevenge', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvSpec(id='MontezumaRevenge-v4', entry_point='ale_py.env.gym:AtariEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'game': 'montezuma_revenge', 'obs_type': 'rgb', 'repeat_action_probability': 0.0, 'full_action_space': False, 'max_num_frames_per_episode': 108000, 'frameskip': (2, 5), 'render_mode': 'rgb_array'}, namespace=None, name='MontezumaRevenge', version=4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_range = (-2, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env1 = GrayScaleObservation(env, keep_dim=True)\n",
    "#env1 = make_vec_env(lambda: env1, n_envs=4) you wouldn't use the dummyvec if you uncomment this\n",
    "env1 = DummyVecEnv([lambda: env1]) #create a vectorized environment  for parallelized training using multiole envs\n",
    "env1 = VecFrameStack(env1, 4, channels_order='last') #consecutive frames are stacked together as a single input to the agent's policy network to make decisions based on the temporal dynamics of the game env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state(state):\n",
    "    # Extract dimensions of a single frame\n",
    "    state = state.squeeze()\n",
    "    height, width, num_frames = state.shape\n",
    "\n",
    "    # New dimensions for downscaling \n",
    "    new_width = 8\n",
    "    new_height = 11\n",
    "    depth = 12  \n",
    "\n",
    "    # Resize each frame individually\n",
    "    resized_frames = []\n",
    "    for i in range(num_frames):\n",
    "        resized_frame = cv2.resize(state[:, :, i], (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "        resized_frame = ((resized_frame / 255.0) * depth).astype(np.uint8)\n",
    "        resized_frames.append(resized_frame)\n",
    "    \n",
    "    # Stack the resized frames back together\n",
    "    resized_state = np.stack(resized_frames, axis=-1)\n",
    "\n",
    "    return resized_state.astype(np.uint8)\n",
    "\n",
    "def make_reference(cell):\n",
    "      cell_as_string = ''.join(cell.astype(int).astype(str).flatten())\n",
    "      cell_as_bytes = cell_as_string.encode()\n",
    "      cell_as_hash_bytes = sha256(cell_as_bytes)\n",
    "      cell_as_hash_hex = cell_as_hash_bytes.hexdigest()\n",
    "      cell_as_hash_int = int(cell_as_hash_hex, 16)\n",
    "      cell_as_hash_string = str(cell_as_hash_int)\n",
    "      return cell_as_hash_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if re.match(r'expl_ppo_montezuma_\\d+_steps.zip', f)]\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    checkpoint_files.sort(key=lambda x: int(re.findall(r'(\\d+)_steps', x)[0]), reverse=True)\n",
    "    return os.path.join(checkpoint_dir, checkpoint_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCallback(BaseCallback):\n",
    "    def __init__(self, verbose=1, log_file='expl_training_log.csv', log_file2='expl_episode_info.csv'):\n",
    "        super(LogCallback, self).__init__(verbose)\n",
    "        self.visited_cells = set()\n",
    "        self.visited_cells_per_episode = set()\n",
    "        self.current_episode_reward = 0\n",
    "        self.score = 0\n",
    "        self.episode_count = 0\n",
    "        self.iteration_count = 0  # Added iteration count\n",
    "        self.log_file = log_file\n",
    "        self.log_file2 = log_file2\n",
    "        self.train_keys = ['train/entropy_loss', 'train/policy_gradient_loss', \n",
    "                           'train/value_loss', 'train/approx_kl', 'train/clip_fraction', \n",
    "                           'train/loss', 'train/explained_variance']\n",
    "        self.metrics = []\n",
    "\n",
    "        # Ensure the log files have headers\n",
    "        self._initialize_log_files()\n",
    "\n",
    "    def _initialize_log_files(self):\n",
    "        # Check if log_file exists, if not, write headers\n",
    "        if not os.path.exists(self.log_file):\n",
    "            with open(self.log_file, 'w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Iteration', 'Episode', 'Cells', 'Score'] + self.train_keys)\n",
    "\n",
    "        # Check if log_file2 exists, if not, write headers\n",
    "        if not os.path.exists(self.log_file2):\n",
    "            with open(self.log_file2, 'w', newline='') as ep_file:\n",
    "                writer = csv.writer(ep_file)\n",
    "                writer.writerow(['Episode', 'Cells per ep', 'Reward per ep', 'Score'])\n",
    "\n",
    "    def save_state(self, save_path):\n",
    "        state = {\n",
    "            'visited_cells': self.visited_cells,\n",
    "            'visited_cells_per_episode': self.visited_cells_per_episode,\n",
    "            'current_episode_reward': self.current_episode_reward,\n",
    "            'score': self.score,\n",
    "            'episode_count': self.episode_count,\n",
    "            'iteration_count': self.iteration_count  # Save iteration count\n",
    "        }\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        print(f\"LogCallback state saved to {save_path}\")\n",
    "\n",
    "    def load_state(self, load_path):\n",
    "        with open(load_path, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "        self.visited_cells = state['visited_cells']\n",
    "        self.visited_cells_per_episode = state['visited_cells_per_episode']\n",
    "        self.current_episode_reward = state['current_episode_reward']\n",
    "        self.score = state['score']\n",
    "        self.episode_count = state['episode_count']\n",
    "        self.iteration_count = state['iteration_count']  # Load iteration count\n",
    "        print(f\"LogCallback state loaded from {load_path}\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        obs = self.locals['new_obs']\n",
    "        cell = convert_state(obs) \n",
    "        ref = make_reference(cell)\n",
    "        self.visited_cells.add(ref)\n",
    "        self.visited_cells_per_episode.add(ref)\n",
    "        \n",
    "        reward = self.locals['rewards'][0]\n",
    "        self.current_episode_reward += reward\n",
    "        self.score += reward\n",
    "\n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_count += 1\n",
    "\n",
    "            # Write episode information\n",
    "            with open(self.log_file2, 'a', newline='') as ep_file:\n",
    "                writer = csv.writer(ep_file)\n",
    "                writer.writerow([\n",
    "                    self.episode_count,\n",
    "                    len(self.visited_cells_per_episode),\n",
    "                    self.current_episode_reward,\n",
    "                    self.score\n",
    "                ])\n",
    "     \n",
    "            self.current_episode_reward = 0\n",
    "            self.visited_cells_per_episode.clear()\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        metrics = {key: self.model.logger.name_to_value.get(key, 0.0) for key in self.train_keys}\n",
    "        self.metrics.append(metrics)\n",
    "        self.iteration_count += 1  # Increment iteration count\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(f\"Iteration {self.iteration_count}: {metrics}\")\n",
    "            print(f\"Episode {self.episode_count}, Cells Discovered: {len(self.visited_cells)}, Score: {self.score}\")\n",
    "        \n",
    "        # Write metrics to CSV\n",
    "        with open(self.log_file, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\n",
    "                self.iteration_count,  # Use iteration count\n",
    "                self.episode_count, \n",
    "                len(self.visited_cells), \n",
    "                self.score\n",
    "            ] + [metrics.get(key, 0.0) for key in self.train_keys])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCheckpointCallback(CheckpointCallback):\n",
    "    def __init__(self, start_step=0, log_callback=None, *args, **kwargs):\n",
    "        super(CustomCheckpointCallback, self).__init__(*args, **kwargs)\n",
    "        self.start_step = start_step\n",
    "        self.log_callback = log_callback\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Calculate the total number of steps considering the start step\n",
    "        total_steps = self.num_timesteps + self.start_step\n",
    "\n",
    "        # Save the model and log callback state if the condition is met\n",
    "        if total_steps % self.save_freq == 0:\n",
    "            save_path = os.path.join(self.save_path, f\"{self.name_prefix}_{total_steps}_steps.zip\")\n",
    "            self.model.save(save_path)\n",
    "            print(f\"Saving model checkpoint to {save_path}\")\n",
    "\n",
    "            # Save LogCallback state to a single file\n",
    "            if self.log_callback is not None:\n",
    "                log_state_path = os.path.join(self.save_path, 'expl_log_callback_state.pkl')\n",
    "                self.log_callback.save_state(log_state_path)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Starting new training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./exploration_ppo_models/expl_ppo_montezuma_2048_steps.zip\n",
      "LogCallback state saved to ./exploration_ppo_models/expl_log_callback_state.pkl\n",
      "Iteration 1: {'train/entropy_loss': 0.0, 'train/policy_gradient_loss': 0.0, 'train/value_loss': 0.0, 'train/approx_kl': 0.0, 'train/clip_fraction': 0.0, 'train/loss': 0.0, 'train/explained_variance': 0.0}\n",
      "Episode 3, Cells Discovered: 1127, Score: 0.0\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 58   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 34   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "Saving model checkpoint to ./exploration_ppo_models/expl_ppo_montezuma_4096_steps.zip\n",
      "LogCallback state saved to ./exploration_ppo_models/expl_log_callback_state.pkl\n",
      "Iteration 2: {'train/entropy_loss': -2.881313696503639, 'train/policy_gradient_loss': -0.008883864250674379, 'train/value_loss': 0.11371231580565108, 'train/approx_kl': 0.014335005, 'train/clip_fraction': 0.075634765625, 'train/loss': -0.05569346994161606, 'train/explained_variance': -2.3275935649871826}\n",
      "Episode 5, Cells Discovered: 2397, Score: 0.0\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014335005 |\n",
      "|    clip_fraction        | 0.0756      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | -2.33       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0557     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00888    |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./exploration_ppo_models/expl_ppo_montezuma_6144_steps.zip\n",
      "LogCallback state saved to ./exploration_ppo_models/expl_log_callback_state.pkl\n",
      "Iteration 3: {'train/entropy_loss': -2.8710663177073004, 'train/policy_gradient_loss': -0.0177037350833416, 'train/value_loss': 0.0007445418417091787, 'train/approx_kl': 0.010535579, 'train/clip_fraction': 0.0931640625, 'train/loss': -0.06100688502192497, 'train/explained_variance': -0.7230501174926758}\n",
      "Episode 8, Cells Discovered: 3499, Score: 0.0\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 264         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010535579 |\n",
      "|    clip_fraction        | 0.0932      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -0.723      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.061      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 0.000745    |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./exploration_ppo_models/expl_ppo_montezuma_8192_steps.zip\n",
      "LogCallback state saved to ./exploration_ppo_models/expl_log_callback_state.pkl\n",
      "Iteration 4: {'train/entropy_loss': -2.8686529703438284, 'train/policy_gradient_loss': -0.04257960504328366, 'train/value_loss': 0.0010324546908123012, 'train/approx_kl': 0.023575936, 'train/clip_fraction': 0.171435546875, 'train/loss': -0.11411397904157639, 'train/explained_variance': -1.799731969833374}\n",
      "Episode 11, Cells Discovered: 4817, Score: 0.0\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 20          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 393         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023575936 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -1.8        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.114      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    value_loss           | 0.00103     |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ensure the checkpoint directory exists\n",
    "checkpoint_dir_expl = './exploration_ppo_models/'\n",
    "os.makedirs(checkpoint_dir_expl, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "latest_checkpoint = get_latest_checkpoint(checkpoint_dir_expl)\n",
    "state_file_path = os.path.join(checkpoint_dir_expl, 'expl_log_callback_state.pkl')\n",
    "\n",
    "if latest_checkpoint:\n",
    "    model2 = PPO.load(latest_checkpoint, env=env1)\n",
    "    last_checkpoint_step = int(re.findall(r'(\\d+)_steps', latest_checkpoint)[0])\n",
    "    print(f\"Resuming training from checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    log_callback = LogCallback()\n",
    "    if os.path.exists(state_file_path):\n",
    "        log_callback.load_state(state_file_path)\n",
    "    else:\n",
    "        print(\"No previous log callback state found. Starting fresh.\")\n",
    "else:\n",
    "    model2 = PPO('CnnPolicy', env1, learning_rate=2.5e-4, gamma=0.99, ent_coef=0.01, verbose=1)\n",
    "    last_checkpoint_step = 0\n",
    "    log_callback = LogCallback()\n",
    "    print(\"Starting new training\")\n",
    "\n",
    "# Calculate the remaining timesteps to train\n",
    "total_timesteps = 10000000\n",
    "remaining_timesteps = total_timesteps - last_checkpoint_step\n",
    "\n",
    "# Create the checkpoint callback\n",
    "checkpoint_callback = CustomCheckpointCallback(\n",
    "    start_step=last_checkpoint_step,\n",
    "    save_freq=2048,\n",
    "    save_path=checkpoint_dir_expl,\n",
    "    name_prefix='expl_ppo_montezuma',\n",
    "    log_callback=log_callback\n",
    ")\n",
    "\n",
    "# Train the model with the callbacks\n",
    "model2.learn(total_timesteps=remaining_timesteps, callback=[log_callback, checkpoint_callback])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
