{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import csv\n",
    "import re \n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import cv2\n",
    "from hashlib import sha256\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecEnvWrapper\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:563: UserWarning: \u001b[33mWARN: Using the latest versioned environment `MontezumaRevenge-v4` instead of the unversioned environment `MontezumaRevenge`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = gym.make('MontezumaRevenge', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(18)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvSpec(id='MontezumaRevenge-v4', entry_point='ale_py.env.gym:AtariEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'game': 'montezuma_revenge', 'obs_type': 'rgb', 'repeat_action_probability': 0.0, 'full_action_space': False, 'max_num_frames_per_episode': 108000, 'frameskip': (2, 5), 'render_mode': 'rgb_array'}, namespace=None, name='MontezumaRevenge', version=4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_range = (-2, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env1 = GrayScaleObservation(env, keep_dim=True)\n",
    "#env1 = make_vec_env(lambda: env1, n_envs=4) you wouldn't use the dummyvec if you uncomment this\n",
    "env1 = DummyVecEnv([lambda: env1]) #create a vectorized environment  for parallelized training using multiole envs\n",
    "env1 = VecFrameStack(env1, 4, channels_order='last') #consecutive frames are stacked together as a single input to the agent's policy network to make decisions based on the temporal dynamics of the game env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state(state):\n",
    "    # Extract dimensions of a single frame\n",
    "    state = state.squeeze()\n",
    "    height, width, num_frames = state.shape\n",
    "\n",
    "    # New dimensions for downscaling \n",
    "    new_width = 8\n",
    "    new_height = 11\n",
    "    depth = 12  \n",
    "\n",
    "    # Resize each frame individually\n",
    "    resized_frames = []\n",
    "    for i in range(num_frames):\n",
    "        resized_frame = cv2.resize(state[:, :, i], (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "        resized_frame = ((resized_frame / 255.0) * depth).astype(np.uint8)\n",
    "        resized_frames.append(resized_frame)\n",
    "    \n",
    "    # Stack the resized frames back together\n",
    "    resized_state = np.stack(resized_frames, axis=-1)\n",
    "\n",
    "    return resized_state.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reference(cell):\n",
    "      cell_as_string = ''.join(cell.astype(int).astype(str).flatten())\n",
    "      cell_as_bytes = cell_as_string.encode()\n",
    "      cell_as_hash_bytes = sha256(cell_as_bytes)\n",
    "      cell_as_hash_hex = cell_as_hash_bytes.hexdigest()\n",
    "      cell_as_hash_int = int(cell_as_hash_hex, 16)\n",
    "      cell_as_hash_string = str(cell_as_hash_int)\n",
    "      return cell_as_hash_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if re.match(r'no_expl_ppo_montezuma_\\d+_steps.zip', f)]\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    checkpoint_files.sort(key=lambda x: int(re.findall(r'(\\d+)_steps', x)[0]), reverse=True)\n",
    "    return os.path.join(checkpoint_dir, checkpoint_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCallback(BaseCallback):\n",
    "    def __init__(self, verbose=1, log_file='training_log.csv', log_file2='episode_info.csv'):\n",
    "        super(LogCallback, self).__init__(verbose)\n",
    "        self.visited_cells = set()\n",
    "        self.visited_cells_per_episode = set()\n",
    "        self.current_episode_reward = 0\n",
    "        self.score = 0\n",
    "        self.episode_count = 0\n",
    "        self.iteration_count = 0  # Added iteration count\n",
    "        self.log_file = log_file\n",
    "        self.log_file2 = log_file2\n",
    "        self.train_keys = ['train/entropy_loss', 'train/policy_gradient_loss', \n",
    "                           'train/value_loss', 'train/approx_kl', 'train/clip_fraction', \n",
    "                           'train/loss', 'train/explained_variance']\n",
    "        self.metrics = []\n",
    "\n",
    "        # Ensure the log files have headers\n",
    "        self._initialize_log_files()\n",
    "\n",
    "    def _initialize_log_files(self):\n",
    "        # Check if log_file exists, if not, write headers\n",
    "        if not os.path.exists(self.log_file):\n",
    "            with open(self.log_file, 'w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Iteration', 'Episode', 'Cells', 'Score'] + self.train_keys)\n",
    "\n",
    "        # Check if log_file2 exists, if not, write headers\n",
    "        if not os.path.exists(self.log_file2):\n",
    "            with open(self.log_file2, 'w', newline='') as ep_file:\n",
    "                writer = csv.writer(ep_file)\n",
    "                writer.writerow(['Episode', 'Cells per ep', 'Reward per ep', 'Score'])\n",
    "\n",
    "    def save_state(self, save_path):\n",
    "        state = {\n",
    "            'visited_cells': self.visited_cells,\n",
    "            'visited_cells_per_episode': self.visited_cells_per_episode,\n",
    "            'current_episode_reward': self.current_episode_reward,\n",
    "            'score': self.score,\n",
    "            'episode_count': self.episode_count,\n",
    "            'iteration_count': self.iteration_count  # Save iteration count\n",
    "        }\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        print(f\"LogCallback state saved to {save_path}\")\n",
    "\n",
    "    def load_state(self, load_path):\n",
    "        with open(load_path, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "        self.visited_cells = state['visited_cells']\n",
    "        self.visited_cells_per_episode = state['visited_cells_per_episode']\n",
    "        self.current_episode_reward = state['current_episode_reward']\n",
    "        self.score = state['score']\n",
    "        self.episode_count = state['episode_count']\n",
    "        self.iteration_count = state['iteration_count']  # Load iteration count\n",
    "        print(f\"LogCallback state loaded from {load_path}\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        obs = self.locals['new_obs']\n",
    "        cell = convert_state(obs) \n",
    "        ref = make_reference(cell)\n",
    "        self.visited_cells.add(ref)\n",
    "        self.visited_cells_per_episode.add(ref)\n",
    "        \n",
    "        reward = self.locals['rewards'][0]\n",
    "        self.current_episode_reward += reward\n",
    "        self.score += reward\n",
    "\n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_count += 1\n",
    "\n",
    "            # Write episode information\n",
    "            with open(self.log_file2, 'a', newline='') as ep_file:\n",
    "                writer = csv.writer(ep_file)\n",
    "                writer.writerow([\n",
    "                    self.episode_count,\n",
    "                    len(self.visited_cells_per_episode),\n",
    "                    self.current_episode_reward,\n",
    "                    self.score\n",
    "                ])\n",
    "     \n",
    "            self.current_episode_reward = 0\n",
    "            self.visited_cells_per_episode.clear()\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        metrics = {key: self.model.logger.name_to_value.get(key, 0.0) for key in self.train_keys}\n",
    "        self.metrics.append(metrics)\n",
    "        self.iteration_count += 1  # Increment iteration count\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(f\"Iteration {self.iteration_count}: {metrics}\")\n",
    "            print(f\"Episode {self.episode_count}, Cells Discovered: {len(self.visited_cells)}, Score: {self.score}\")\n",
    "        \n",
    "        # Write metrics to CSV\n",
    "        with open(self.log_file, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\n",
    "                self.iteration_count,  # Use iteration count\n",
    "                self.episode_count, \n",
    "                len(self.visited_cells), \n",
    "                self.score\n",
    "            ] + [metrics.get(key, 0.0) for key in self.train_keys])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCheckpointCallback(CheckpointCallback):\n",
    "    def __init__(self, start_step=0, log_callback=None, *args, **kwargs):\n",
    "        super(CustomCheckpointCallback, self).__init__(*args, **kwargs)\n",
    "        self.start_step = start_step\n",
    "        self.log_callback = log_callback\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Calculate the total number of steps considering the start step\n",
    "        total_steps = self.num_timesteps + self.start_step\n",
    "\n",
    "        # Save the model and log callback state if the condition is met\n",
    "        if total_steps % self.save_freq == 0:\n",
    "            save_path = os.path.join(self.save_path, f\"{self.name_prefix}_{total_steps}_steps.zip\")\n",
    "            self.model.save(save_path)\n",
    "            print(f\"Saving model checkpoint to {save_path}\")\n",
    "\n",
    "            # Save LogCallback state to a single file\n",
    "            if self.log_callback is not None:\n",
    "                log_state_path = os.path.join(self.save_path, 'log_callback_state.pkl')\n",
    "                self.log_callback.save_state(log_state_path)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n",
      "Resuming training from checkpoint: ./no_exploration_ppo_models/no_expl_ppo_montezuma_53248_steps.zip\n",
      "LogCallback state loaded from ./no_exploration_ppo_models/log_callback_state.pkl\n",
      "Saving model checkpoint to ./no_exploration_ppo_models/no_expl_ppo_montezuma_55296_steps.zip\n",
      "LogCallback state saved to ./no_exploration_ppo_models/log_callback_state.pkl\n",
      "Iteration 20: {'train/entropy_loss': 0.0, 'train/policy_gradient_loss': 0.0, 'train/value_loss': 0.0, 'train/approx_kl': 0.0, 'train/clip_fraction': 0.0, 'train/loss': 0.0, 'train/explained_variance': 0.0}\n",
      "Episode 62, Cells Discovered: 24871, Score: 0.0\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 51   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 39   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 39\u001b[0m\n\u001b[0;32m     30\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m CustomCheckpointCallback(\n\u001b[0;32m     31\u001b[0m     start_step\u001b[38;5;241m=\u001b[39mlast_checkpoint_step,\n\u001b[0;32m     32\u001b[0m     save_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     log_callback\u001b[38;5;241m=\u001b[39mlog_callback\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Train the model with the callbacks\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m model1\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mremaining_timesteps, callback\u001b[38;5;241m=\u001b[39m[log_callback, checkpoint_callback])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m    316\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    317\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    318\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[0;32m    319\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[0;32m    320\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    321\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:313\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    315\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:279\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 279\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[0;32m    281\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensure the checkpoint directory exists\n",
    "checkpoint_dir = './no_exploration_ppo_models/'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "latest_checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "state_file_path = os.path.join(checkpoint_dir, 'log_callback_state.pkl')\n",
    "\n",
    "if latest_checkpoint:\n",
    "    model1 = PPO.load(latest_checkpoint, env=env1)\n",
    "    last_checkpoint_step = int(re.findall(r'(\\d+)_steps', latest_checkpoint)[0])\n",
    "    print(f\"Resuming training from checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    log_callback = LogCallback()\n",
    "    if os.path.exists(state_file_path):\n",
    "        log_callback.load_state(state_file_path)\n",
    "    else:\n",
    "        print(\"No previous log callback state found. Starting fresh.\")\n",
    "else:\n",
    "    model1 = PPO('CnnPolicy', env1, learning_rate=2.5e-4, gamma=0.99, verbose=1)\n",
    "    last_checkpoint_step = 0\n",
    "    log_callback = LogCallback()\n",
    "    print(\"Starting new training\")\n",
    "\n",
    "# Calculate the remaining timesteps to train\n",
    "total_timesteps = 10000000\n",
    "remaining_timesteps = total_timesteps - last_checkpoint_step\n",
    "\n",
    "# Create the checkpoint callback\n",
    "checkpoint_callback = CustomCheckpointCallback(\n",
    "    start_step=last_checkpoint_step,\n",
    "    save_freq=2048,\n",
    "    save_path=checkpoint_dir,\n",
    "    name_prefix='no_expl_ppo_montezuma',\n",
    "    log_callback=log_callback\n",
    ")\n",
    "\n",
    "# Train the model with the callbacks\n",
    "model1.learn(total_timesteps=remaining_timesteps, callback=[log_callback, checkpoint_callback])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
